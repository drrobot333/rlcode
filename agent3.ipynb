{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "massive-norman",
   "metadata": {},
   "source": [
    "## DRL 코드입니다.\n",
    "* 저는 이 코드와 제가 만든 환경을 가지고 학습을 했지만, 실제로는 OMNeT++와 이 이 코드를 연동시켜주셔야 합니다.\n",
    "* 그러므로 제가 만든 환경 코드는 이해하실 필요가 없습니다. 이 코드의 DNN 구조와, 환경과 상호작용하는 인터페이스 부분만 참고해주시면 될 듯 합니다.\n",
    "* main 부분은 아키텍처 설명한 ppt를 보시고 새로 짜시는게 더 빠를 수 있습니다.\n",
    "* DQN based가 아닌, Policy gradient based인 actor-critic을 쓰고자 합니다(많은 스케줄링 관련 논문에서 actor-critic 사용. 또한 DQN은 규모가 좀 커지면 학습 힘들다고 알고 있음).\n",
    "* actor-critic 알고리즘은 다양합니다. 저희는 A3C 혹은 PPO를 쓸 것 같습니다. 각 알고리즘에 대해선 논문을 읽어보시거나 검색하셔서 이해해주시면 될 것 같습니다.\n",
    "* actor network로 GNN(Graph Neural Network)를 사용합니다. Pytorch geometric library에서 다양한 GNN 모듈을 제공합니다. 저희는 Dynamic edge-conditioned GNN을 씁니다.\n",
    "* Dynamic edge-conditioned GNN : https://arxiv.org/abs/1704.02901\n",
    "* 코드 기반 : https://github.com/seungeunrho/minimalRL/blob/master/ppo.py\n",
    "* GNN 예제 : https://baeseongsu.github.io/posts/pytorch-geometric-introduction/\n",
    "* GNN 배치 단위 inference 하는법 : https://pytorch-geometric.readthedocs.io/en/latest/notes/batching.html\n",
    "* 지금 이 코드는 돌아가긴 하지만, 학습이 제대로 안되는 상황입니다. 모든 state에 대해서 한 액션으로 거의 모든 확률이 쏠려버리게끔 학습이 되는데, 원인은 찾지 못했습니다. 아마 제가 만든 환경에 문제가 있을 수도 있고, 학습 자체를 더 튜닝해야 될 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03b4acf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "from library import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch_geometric.nn import NNConv, global_mean_pool, GraphUNet\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87a5ce44",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate   = 0.001\n",
    "gamma           = 0.9\n",
    "entropy_weight  = 0.1\n",
    "'''\n",
    "entropy_weight : loss함수에 entropy라는 걸 더해주는 테크닉에서의 weight입니다.\n",
    "이 기법은 A3C 논문에 나와있으며, 쓰는 이유는 exploration을 촉진하기 위해서입니다.\n",
    "각 액션을 취할 확률을 거의 균등하게끔 업데이트해줌으로써 다양한 액션을 해볼 기회가 많아집니다.\n",
    "actor-critic에서 이 기법으로 인한 성능 개선이 매우 효과적인 것으로 알고있으므로, 적용해주시면 좋을 듯 합니다.\n",
    "'''\n",
    "lmbda         = 0.95\n",
    "eps_clip      = 0.1\n",
    "'''\n",
    "위 두 인자는 PPO에서 씁니다.\n",
    "'''\n",
    "\n",
    "loss_coef = 0.5\n",
    "\n",
    "node_feature_num = 100\n",
    "queue_feature_num = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01c1fbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class actor_network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(actor_network, self).__init__()\n",
    "        self.data = []\n",
    "        self.replay_memory = []\n",
    "\n",
    "        self.pi_mlp1 = nn.Sequential(nn.Linear(1, node_feature_num * node_feature_num), nn.ReLU())\n",
    "        self.pi_s_gcn = NNConv(node_feature_num, node_feature_num, self.pi_mlp1, aggr='mean')\n",
    "\n",
    "        self.pi_graph_u_net = GraphUNet(node_feature_num, 10, node_feature_num, 4, 0.8)\n",
    "\n",
    "        self.pi_backbone = nn.Sequential(\n",
    "            nn.Linear(200, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # prob_fc : 각 액션에 대한 확률.\n",
    "        self.pi_prob_fc = nn.Linear(64, 5)\n",
    "\n",
    "        self.v_mlp1 = nn.Sequential(nn.Linear(1, node_feature_num * node_feature_num), nn.ReLU())\n",
    "        self.v_s_gcn = NNConv(node_feature_num, node_feature_num, self.v_mlp1, aggr='mean')\n",
    "\n",
    "        self.v_graph_u_net = GraphUNet(node_feature_num, 10, node_feature_num, 4, 0.8)\n",
    "\n",
    "        self.v_backbone = nn.Sequential(\n",
    "            nn.Linear(200, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # prob_fc : 각 액션에 대한 확률.\n",
    "        self.v_value_fc = nn.Linear(64, 1)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "              \n",
    "        \n",
    "    # policy DNN\n",
    "    def pi(self, state):\n",
    "        data, job_waiting_feature = state # data = graph data\n",
    "        node_feature, link_feature, adjacency = data.x, data.edge_attr, data.edge_index\n",
    "        \"\"\"\n",
    "        node_feature = F.relu(self.conv1(node_feature, adjacency, link_feature))\n",
    "        node_feature = F.relu(self.conv2(node_feature, adjacency, link_feature))\n",
    "        #node_feature = F.relu(self.conv3(node_feature, adjacency, link_feature))\n",
    "        readout = global_mean_pool(node_feature, data.batch) # 모든 노드의 feature를 평균내서 하나의 벡터로 만들어주기.\n",
    "        \"\"\"\n",
    "\n",
    "        node_feature = F.relu(self.pi_s_gcn(node_feature, adjacency, link_feature))\n",
    "        node_feature = self.pi_graph_u_net(node_feature, adjacency)\n",
    "        readout = global_mean_pool(node_feature, data.batch)\n",
    "        concat = torch.cat([readout, job_waiting_feature], dim=1) # 여기에 job waiting 벡터 붙이기.\n",
    "\n",
    "        feature_extract = self.pi_backbone(concat)\n",
    "\n",
    "        output = self.pi_prob_fc(feature_extract)\n",
    "\n",
    "        prob = F.softmax(output, dim=1)\n",
    "        \n",
    "        # 아래는 엔트로피 구하는 과정\n",
    "        log_prob = F.log_softmax(output, dim=1)\n",
    "        entropy = (log_prob * prob).sum(1, keepdim=True)\n",
    "        return prob, entropy      \n",
    "\n",
    "    # advantage network\n",
    "    def v(self, state):\n",
    "        data, job_waiting_feature = state\n",
    "        node_feature, link_feature, adjacency = data.x, data.edge_attr, data.edge_index\n",
    "        \"\"\"node_feature = F.relu(self.conv1(node_feature, adjacency, link_feature))\n",
    "        node_feature = F.relu(self.conv2(node_feature, adjacency, link_feature))\n",
    "        #node_feature = F.relu(self.conv3(node_feature, adjacency, link_feature))\n",
    "        readout = global_mean_pool(node_feature, data.batch)\n",
    "        \n",
    "        # job waiting vector concat\n",
    "        concat = torch.cat([readout, job_waiting_feature], dim=1)\"\"\"\n",
    "\n",
    "        node_feature = F.relu(self.v_s_gcn(node_feature, adjacency, link_feature))\n",
    "        node_feature = self.v_graph_u_net(node_feature, adjacency)\n",
    "        readout = global_mean_pool(node_feature, data.batch)\n",
    "        concat = torch.cat([readout, job_waiting_feature], dim=1) # 여기에 job waiting 벡터 붙이기.\n",
    "\n",
    "        feature_extract = self.v_backbone(concat)\n",
    "        \n",
    "        value = self.v_value_fc(feature_extract) # 앞부분은 pi랑 공유해야 하고, concat -> value_fc를 거치는 것만 다름.\n",
    "        return value\n",
    "        \n",
    "    def put_data(self, transition):\n",
    "        self.data.append(transition)\n",
    "        \n",
    "    \n",
    "    # make_batch, train_net은 맨 위에 코드 기반 링크와 거의 동일합니다.\n",
    "        \n",
    "    def make_batch(self):\n",
    "        network_lst, job_waiting_lst, a_lst, r_lst, next_network_lst, next_job_waiting_lst = [], [], [], [], [], []\n",
    "        sampled_memory = random.sample(self.replay_memory, 32)\n",
    "\n",
    "        for transition in sampled_memory:\n",
    "            network, job_waiting, a, r, next_network, next_job_waiting = transition\n",
    "\n",
    "            network_lst.append(network)\n",
    "            job_waiting_lst.append(job_waiting)\n",
    "            a_lst.append([a])\n",
    "            r_lst.append([r])\n",
    "            next_network_lst.append(next_network)\n",
    "            next_job_waiting_lst.append(next_job_waiting)\n",
    "            \n",
    "        \n",
    "        # gnn sample을 배치단위로 inference하려면 이렇게 묶어줘야 함.\n",
    "        network_loader = DataLoader(network_lst, batch_size=len(network_lst))\n",
    "        next_network_loader = DataLoader(next_network_lst, batch_size=len(next_network_lst))\n",
    "        network_batch = next(iter(network_loader))\n",
    "        next_network_batch = next(iter(next_network_loader))\n",
    "        \n",
    "        job_waiting = torch.tensor(np.array(job_waiting_lst), dtype=torch.float)\n",
    "        a = torch.tensor(a_lst)\n",
    "        r = torch.tensor(r_lst, dtype=torch.float)\n",
    "        next_job_waiting = torch.tensor(np.array(next_job_waiting_lst), dtype=torch.float)\n",
    "        \n",
    "        self.data = []\n",
    "        return network_batch, job_waiting, a, r, next_network_batch, next_job_waiting\n",
    "    \n",
    "    \n",
    "    \n",
    "    def train_net(self):\n",
    "        \n",
    "        for i in range(5):\n",
    "            network_batch, job_waiting, a, r, next_network_batch, next_job_waiting = self.make_batch()\n",
    "            print(network_batch)\n",
    "\n",
    "            q_out = self.pi([network_batch, job_waiting])[0]\n",
    "            print(q_out)\n",
    "            q_a = q_out.gather(1, a)\n",
    "            \n",
    "            max_q_prime = self.pi([network_batch, job_waiting]).max(1)[0].unsqueeze(1)\n",
    "            target = r * gamma * max_q_prime\n",
    "            loss = F.smooth_l1_loss(q_a, target)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "768ca02f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[160, 100], edge_index=[2, 448], edge_attr=[448, 1], batch=[160], ptr=[33])\n",
      "tensor([0.2022, 0.1995, 0.2213, 0.1787, 0.1984], grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\user\\Desktop\\suhwan\\학부연구생\\RL code\\agent3.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 123>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/suhwan/%ED%95%99%EB%B6%80%EC%97%B0%EA%B5%AC%EC%83%9D/RL%20code/agent3.ipynb#ch0000004?line=117'>118</a>\u001b[0m             env\u001b[39m.\u001b[39mmove_job()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/suhwan/%ED%95%99%EB%B6%80%EC%97%B0%EA%B5%AC%EC%83%9D/RL%20code/agent3.ipynb#ch0000004?line=122'>123</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/suhwan/%ED%95%99%EB%B6%80%EC%97%B0%EA%B5%AC%EC%83%9D/RL%20code/agent3.ipynb#ch0000004?line=123'>124</a>\u001b[0m     main()\n",
      "\u001b[1;32mc:\\Users\\user\\Desktop\\suhwan\\학부연구생\\RL code\\agent3.ipynb Cell 5'\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/suhwan/%ED%95%99%EB%B6%80%EC%97%B0%EA%B5%AC%EC%83%9D/RL%20code/agent3.ipynb#ch0000004?line=109'>110</a>\u001b[0m model\u001b[39m.\u001b[39mreplay_memory \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mreplay_memory[\u001b[39m-\u001b[39m\u001b[39m5000\u001b[39m:]\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/suhwan/%ED%95%99%EB%B6%80%EC%97%B0%EA%B5%AC%EC%83%9D/RL%20code/agent3.ipynb#ch0000004?line=111'>112</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(model\u001b[39m.\u001b[39mdata) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m time \u001b[39m%\u001b[39m \u001b[39m1000\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/suhwan/%ED%95%99%EB%B6%80%EC%97%B0%EA%B5%AC%EC%83%9D/RL%20code/agent3.ipynb#ch0000004?line=112'>113</a>\u001b[0m     model\u001b[39m.\u001b[39;49mtrain_net()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/suhwan/%ED%95%99%EB%B6%80%EC%97%B0%EA%B5%AC%EC%83%9D/RL%20code/agent3.ipynb#ch0000004?line=114'>115</a>\u001b[0m \u001b[39m# 새로운 job을 발생시키는 코드.\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/suhwan/%ED%95%99%EB%B6%80%EC%97%B0%EA%B5%AC%EC%83%9D/RL%20code/agent3.ipynb#ch0000004?line=115'>116</a>\u001b[0m \u001b[39mif\u001b[39;00m time\u001b[39m%\u001b[39m\u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;32mc:\\Users\\user\\Desktop\\suhwan\\학부연구생\\RL code\\agent3.ipynb Cell 4'\u001b[0m in \u001b[0;36mactor_network.train_net\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/suhwan/%ED%95%99%EB%B6%80%EC%97%B0%EA%B5%AC%EC%83%9D/RL%20code/agent3.ipynb#ch0000003?line=131'>132</a>\u001b[0m q_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpi([network_batch, job_waiting])[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/suhwan/%ED%95%99%EB%B6%80%EC%97%B0%EA%B5%AC%EC%83%9D/RL%20code/agent3.ipynb#ch0000003?line=132'>133</a>\u001b[0m \u001b[39mprint\u001b[39m(q_out)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/suhwan/%ED%95%99%EB%B6%80%EC%97%B0%EA%B5%AC%EC%83%9D/RL%20code/agent3.ipynb#ch0000003?line=133'>134</a>\u001b[0m q_a \u001b[39m=\u001b[39m q_out\u001b[39m.\u001b[39;49mgather(\u001b[39m1\u001b[39;49m, a)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/suhwan/%ED%95%99%EB%B6%80%EC%97%B0%EA%B5%AC%EC%83%9D/RL%20code/agent3.ipynb#ch0000003?line=135'>136</a>\u001b[0m max_q_prime \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpi([network_batch, job_waiting])\u001b[39m.\u001b[39mmax(\u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/suhwan/%ED%95%99%EB%B6%80%EC%97%B0%EA%B5%AC%EC%83%9D/RL%20code/agent3.ipynb#ch0000003?line=136'>137</a>\u001b[0m target \u001b[39m=\u001b[39m r \u001b[39m*\u001b[39m gamma \u001b[39m*\u001b[39m max_q_prime\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    env = system_manager()\n",
    "    model = actor_network()\n",
    "\n",
    "    reward_history = []\n",
    "    v_history = []\n",
    "    \n",
    "    # network topology의 edges(GNN 예제 링크 참고)\n",
    "    adjacency = torch.tensor([[0, 0, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 4, 4],\n",
    "                              [1, 2, 0, 2, 3, 0, 1, 3, 4, 1, 2, 4, 2, 3]], dtype=torch.long)\n",
    "\n",
    "    # for each time step\n",
    "    time = 0\n",
    "    while True:\n",
    "        #print('---------------------------------------------------------------------------')\n",
    "        node_state, link_state = env.get_state()  # 환경으로부터 실제 state를 관측해 옴(OMNeT++에서 얻어온 statistic로 대체되어야 함).\n",
    "        node_state = torch.tensor(node_state, dtype=torch.float)\n",
    "        link_state = torch.tensor(link_state, dtype=torch.float)\n",
    "        job_waiting_state = env.get_job_waiting_vector()\n",
    "        \n",
    "        env.init_job_progress()\n",
    "\n",
    "        temp_replay_memory = []\n",
    "        \n",
    "        # actions through multiple inferences\n",
    "        scheduled_job_num = 0\n",
    "        # 시스템에 job이 꽉찼거나, 스케줄할 job이 없거나, 현재 timestep에서 스케줄링한 job 개수가 10개 이상이면 그만한다.\n",
    "        while env.activated_job_num < 10 and len(env.job_waiting_queue) > 0 and scheduled_job_num < 10:\n",
    "            # job waiting 제일 앞에 있는 job 가져와서 스케줄링 해야 함.\n",
    "            job_idx = env.assign_index()\n",
    "            job = env.job_waiting_queue[0]\n",
    "            subtasks = job.subtasks\n",
    "            offloading_vector = []\n",
    "            \n",
    "            # 이 job의 모든 subtasks(layers)를 스케줄링해야 함.\n",
    "            for order in range(len(subtasks)):\n",
    "                network_state = Data(x=node_state, edge_attr=link_state, edge_index=adjacency)\n",
    "                prob, entropy = model.pi([network_state, torch.tensor(np.array([job_waiting_state]), dtype=torch.float)])\n",
    "                #print(f'prob : {prob}')\n",
    "                \n",
    "                m = Categorical(prob)\n",
    "                node = m.sample().item()\n",
    "                #print(f'node : {node}')\n",
    "                offloading_vector.append(node)\n",
    "                \n",
    "                # state transition(환경으로부터 매번 state를 업데이트하는게 아닌, 현재 state를 기반으로 action에 해당하는 waiting만 더해줌).\n",
    "                # 아래의 구체적인 코드는 이해하시기 보단 OMNeT++에서 받아온 데이터로 새로 짜시는게 빠를 것 같습니다.\n",
    "                next_node_state = node_state.clone().detach()\n",
    "                next_job_waiting_state = job_waiting_state.copy()\n",
    "                \n",
    "                next_node_state[node][5*job.index+order] += (subtasks[order].comp_demand/100) # 100으로 나누는 이유 : 이렇게 해야 액션이 한쪽 노드로 쏠리게끔 학습이 되는 것을 어느정도 방지할 수 있는 것을 확인.\n",
    "                # 100으로 안나눠주면 너무 큰 값이 state에 추가되어서 inference시 가중치랑 곱해지면서 액션이 한쪽으로 확 쏠리는 걸로 예상됨.\n",
    "                next_network_state = Data(x=next_node_state, edge_attr=link_state, edge_index=adjacency)\n",
    "                next_job_waiting_state[5+order] = 0\n",
    "                \n",
    "                model.put_data([network_state, job_waiting_state, node, 0, next_network_state, next_job_waiting_state, prob[0][node].item(), entropy])\n",
    "\n",
    "                temp_replay_memory.append([network_state, job_waiting_state, node, 0, next_network_state, next_job_waiting_state])\n",
    "                \n",
    "                node_state = next_node_state\n",
    "                job_waiting_state = next_job_waiting_state\n",
    "            \n",
    "            scheduled_job_num += 1\n",
    "            \n",
    "            # job을 시스템에 스케줄링.\n",
    "            env.schedule(job_idx, offloading_vector)\n",
    "            env.move_job() # backlog에 있는 job을 job waiting vector로 옮기는 것.\n",
    "            job_waiting_state = env.get_job_waiting_vector()\n",
    "        \n",
    "        \n",
    "        # 이 timestep이 끝날때까지 시간을 흘려줌(제가 만든 환경이므로 이런 코드가 필요합니다. OMNeT++은 그냥 시뮬레이션 하면서 시간이 자동으로 흐르므로 필요없습니다).\n",
    "        for ms in range(100):\n",
    "            env.step(time)\n",
    "            time += 1\n",
    "        \n",
    "        # 이 timestep이 끝나면, 했던 행동에 대한 reward 계산(이 tiemstep 동안 시스템에 있는 job의 진행정도)\n",
    "        # 지금 보상 계산하는 과정에서 살짝 오류가 있습니다. 보상을 계산하기 전에 이번 timestep에서 어떤 job이 끝나서 없어져 버리면 그 job에 대한 진행정도를 추적할 수 없어서 0이 됩니다.\n",
    "        # 여기서 보상을 계산하는 부분(즉, job의 진행정도 tracking)이 환경과 좀 관련 있기 때문에, 보상을 어떻게 계산할지를 새로 짜시는게 빠를 것 같습니다.\n",
    "        per_timestep_reward = env.get_reward()\n",
    "\n",
    "        #if per_timestep_reward == 0:\n",
    "        #    per_timestep_reward = 5\n",
    "        \n",
    "        #print(f'reward : {per_timestep_reward}')\n",
    "\n",
    "        if time > 100:\n",
    "\n",
    "            reward_history = reward_history[-500:]\n",
    "            reward_history.append(per_timestep_reward)\n",
    "\n",
    "\n",
    "            v_history = v_history[-500:]\n",
    "            v_history.append(int(model.v([network_state, torch.tensor(np.array([job_waiting_state]), dtype=torch.float)])[0]))\n",
    "\n",
    "        if time % 1000 == 0:\n",
    "            plt.subplot(1,2,1)\n",
    "            plt.plot(reward_history)\n",
    "            plt.subplot(1,2,2)\n",
    "            plt.plot(v_history)\n",
    "            plt.show()\n",
    "\n",
    "        \n",
    "        \n",
    "        # 이 timestep에서 발생한 모든 샘플에 똑같은 보상 적용.\n",
    "        for sample in model.data:\n",
    "            sample[3] = per_timestep_reward\n",
    "            temp_replay_memory[3] = per_timestep_reward\n",
    "\n",
    "        model.replay_memory += temp_replay_memory\n",
    "        model.replay_memory = model.replay_memory[-5000:]\n",
    "\n",
    "        if len(model.data) > 0 and time % 1000 == 0:\n",
    "            model.train_net()\n",
    "        \n",
    "        # 새로운 job을 발생시키는 코드.\n",
    "        if time%10 == 0:\n",
    "            env.create_job(time)\n",
    "            env.move_job()\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omnetTest",
   "language": "python",
   "name": "omnettest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
