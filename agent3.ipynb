{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "massive-norman",
   "metadata": {},
   "source": [
    "## DRL 코드입니다.\n",
    "* 저는 이 코드와 제가 만든 환경을 가지고 학습을 했지만, 실제로는 OMNeT++와 이 이 코드를 연동시켜주셔야 합니다.\n",
    "* 그러므로 제가 만든 환경 코드는 이해하실 필요가 없습니다. 이 코드의 DNN 구조와, 환경과 상호작용하는 인터페이스 부분만 참고해주시면 될 듯 합니다.\n",
    "* main 부분은 아키텍처 설명한 ppt를 보시고 새로 짜시는게 더 빠를 수 있습니다.\n",
    "* DQN based가 아닌, Policy gradient based인 actor-critic을 쓰고자 합니다(많은 스케줄링 관련 논문에서 actor-critic 사용. 또한 DQN은 규모가 좀 커지면 학습 힘들다고 알고 있음).\n",
    "* actor-critic 알고리즘은 다양합니다. 저희는 A3C 혹은 PPO를 쓸 것 같습니다. 각 알고리즘에 대해선 논문을 읽어보시거나 검색하셔서 이해해주시면 될 것 같습니다.\n",
    "* actor network로 GNN(Graph Neural Network)를 사용합니다. Pytorch geometric library에서 다양한 GNN 모듈을 제공합니다. 저희는 Dynamic edge-conditioned GNN을 씁니다.\n",
    "* Dynamic edge-conditioned GNN : https://arxiv.org/abs/1704.02901\n",
    "* 코드 기반 : https://github.com/seungeunrho/minimalRL/blob/master/ppo.py\n",
    "* GNN 예제 : https://baeseongsu.github.io/posts/pytorch-geometric-introduction/\n",
    "* GNN 배치 단위 inference 하는법 : https://pytorch-geometric.readthedocs.io/en/latest/notes/batching.html\n",
    "* 지금 이 코드는 돌아가긴 하지만, 학습이 제대로 안되는 상황입니다. 모든 state에 대해서 한 액션으로 거의 모든 확률이 쏠려버리게끔 학습이 되는데, 원인은 찾지 못했습니다. 아마 제가 만든 환경에 문제가 있을 수도 있고, 학습 자체를 더 튜닝해야 될 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b4acf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "from library import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch_geometric.nn import NNConv, global_mean_pool, GraphUNet\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a5ce44",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate   = 0.001\n",
    "gamma           = 0.95\n",
    "entropy_weight  = 0.1\n",
    "'''\n",
    "entropy_weight : loss함수에 entropy라는 걸 더해주는 테크닉에서의 weight입니다.\n",
    "이 기법은 A3C 논문에 나와있으며, 쓰는 이유는 exploration을 촉진하기 위해서입니다.\n",
    "각 액션을 취할 확률을 거의 균등하게끔 업데이트해줌으로써 다양한 액션을 해볼 기회가 많아집니다.\n",
    "actor-critic에서 이 기법으로 인한 성능 개선이 매우 효과적인 것으로 알고있으므로, 적용해주시면 좋을 듯 합니다.\n",
    "'''\n",
    "lmbda         = 0.95\n",
    "eps_clip      = 0.1\n",
    "'''\n",
    "위 두 인자는 PPO에서 씁니다.\n",
    "'''\n",
    "\n",
    "loss_coef = 0.5\n",
    "\n",
    "node_feature_num = 100\n",
    "queue_feature_num = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c1fbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class actor_network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(actor_network, self).__init__()\n",
    "        self.replay_memory = []\n",
    "\n",
    "        self.pi_graph_u_net = GraphUNet(node_feature_num, 20, node_feature_num, 4, 0.9)\n",
    "\n",
    "        self.pi_backbone = nn.Sequential(\n",
    "            nn.Linear(200, 1000),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # prob_fc : 각 액션에 대한 확률.\n",
    "        self.pi_prob_fc = nn.Linear(1000, 5)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "              \n",
    "        \n",
    "    # policy DNN\n",
    "    def pi(self, state):\n",
    "        data, job_waiting_feature = state # data = graph data\n",
    "        job_waiting_feature = job_waiting_feature.view(1, 100)\n",
    "        node_feature, link_feature, adjacency = data.x, data.edge_attr, data.edge_index\n",
    "        \"\"\"\n",
    "        node_feature = F.relu(self.conv1(node_feature, adjacency, link_feature))\n",
    "        node_feature = F.relu(self.conv2(node_feature, adjacency, link_feature))\n",
    "        #node_feature = F.relu(self.conv3(node_feature, adjacency, link_feature))\n",
    "        readout = global_mean_pool(node_feature, data.batch) # 모든 노드의 feature를 평균내서 하나의 벡터로 만들어주기.\n",
    "        \"\"\"\n",
    "\n",
    "        node_feature = self.pi_graph_u_net(node_feature, adjacency)\n",
    "        readout = global_mean_pool(node_feature, data.batch)\n",
    "        #print(\"readout\", readout.shape)\n",
    "        #print(\"job_waiting_feature\", job_waiting_feature.shape)\n",
    "        concat = torch.cat([readout, job_waiting_feature], dim=1) # 여기에 job waiting 벡터 붙이기.\n",
    "\n",
    "        feature_extract = self.pi_backbone(concat)\n",
    "\n",
    "        output = self.pi_prob_fc(feature_extract)\n",
    "        return output     \n",
    "\n",
    "    def put_data(self, transition):\n",
    "        self.data.append(transition)\n",
    "        \n",
    "    \n",
    "    # make_batch, train_net은 맨 위에 코드 기반 링크와 거의 동일합니다.\n",
    "        \n",
    "    def make_batch(self):\n",
    "        network_lst, job_waiting_lst, a_lst, r_lst, next_network_lst, next_job_waiting_lst = [], [], [], [], [], []\n",
    "        sampled_memory = random.sample(self.replay_memory, 32)\n",
    "        \n",
    "        #print(\"sampled_memory\", sampled_memory)\n",
    "\n",
    "        for transition in sampled_memory:\n",
    "            # print(\"transition\", transition)\n",
    "            network, job_waiting, a, r, next_network, next_job_waiting = transition\n",
    "\n",
    "            network_lst.append(network)\n",
    "            job_waiting_lst.append(job_waiting)\n",
    "            a_lst.append([a])\n",
    "            r_lst.append([r])\n",
    "            next_network_lst.append(next_network)\n",
    "            next_job_waiting_lst.append(next_job_waiting)\n",
    "            \n",
    "        # gnn sample을 배치단위로 inference하려면 이렇게 묶어줘야 함.\n",
    "        \n",
    "        return network_lst, torch.tensor(job_waiting_lst, dtype=torch.float), torch.tensor(a_lst, dtype=torch.int64)\\\n",
    "    , torch.tensor(r_lst, dtype=torch.float), next_network_lst, torch.tensor(next_job_waiting_lst, dtype=torch.float)\n",
    "    \n",
    "    \n",
    "    def train_net(self, target_model):\n",
    "        \n",
    "        for i in range(5):\n",
    "            network_batch, job_waiting, a, r, next_network_batch, next_job_waiting = self.make_batch()\n",
    "            for j in range(len(network_batch)):\n",
    "                #print(network_batch[j])\n",
    "\n",
    "                q_out = self.pi([network_batch[j], job_waiting[j]])[0]\n",
    "                \n",
    "                q_a = q_out[a]\n",
    "                \n",
    "                max_q_prime = torch.max(target_model.pi([network_batch[j], job_waiting[j]])[0])\n",
    "                #print(\"max_q_prime\", max_q_prime)\n",
    "                target = r + gamma * max_q_prime\n",
    "                loss = F.smooth_l1_loss(q_a, target)\n",
    "                \n",
    "                #print(\"loss\", loss)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74022172",
   "metadata": {},
   "outputs": [],
   "source": [
    "class target_network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(target_network, self).__init__()\n",
    "        self.data = []\n",
    "        self.replay_memory = []\n",
    "\n",
    "        self.pi_graph_u_net = GraphUNet(node_feature_num, 20, node_feature_num, 4, 0.9)\n",
    "\n",
    "        self.pi_backbone = nn.Sequential(\n",
    "            nn.Linear(200, 1000),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # prob_fc : 각 액션에 대한 확률.\n",
    "        self.pi_prob_fc = nn.Linear(1000, 5)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "              \n",
    "        \n",
    "    # policy DNN\n",
    "    def pi(self, state):\n",
    "        data, job_waiting_feature = state # data = graph data\n",
    "        job_waiting_feature = job_waiting_feature.view(1, 100)\n",
    "        node_feature, link_feature, adjacency = data.x, data.edge_attr, data.edge_index\n",
    "        \"\"\"\n",
    "        node_feature = F.relu(self.conv1(node_feature, adjacency, link_feature))\n",
    "        node_feature = F.relu(self.conv2(node_feature, adjacency, link_feature))\n",
    "        #node_feature = F.relu(self.conv3(node_feature, adjacency, link_feature))\n",
    "        readout = global_mean_pool(node_feature, data.batch) # 모든 노드의 feature를 평균내서 하나의 벡터로 만들어주기.\n",
    "        \"\"\"\n",
    "        node_feature = self.pi_graph_u_net(node_feature, adjacency)\n",
    "        readout = global_mean_pool(node_feature, data.batch)\n",
    "        #print(\"readout\", readout.shape)\n",
    "        #print(\"job_waiting_feature\", job_waiting_feature.shape)\n",
    "        concat = torch.cat([readout, job_waiting_feature], dim=1) # 여기에 job waiting 벡터 붙이기.\n",
    "\n",
    "        feature_extract = self.pi_backbone(concat)\n",
    "\n",
    "        output = self.pi_prob_fc(feature_extract)\n",
    "        return output     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768ca02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = system_manager()\n",
    "model = actor_network()\n",
    "target_model = target_network()\n",
    "reward_history = []\n",
    "v_history = []\n",
    "\n",
    "# network topology의 edges(GNN 예제 링크 참고)\n",
    "adjacency = torch.tensor([[0, 0, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 4, 4],\n",
    "                          [1, 2, 0, 2, 3, 0, 1, 3, 4, 1, 2, 4, 2, 3]], dtype=torch.long)\n",
    "# for each time step\n",
    "time = 0\n",
    "episode = 1\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.98\n",
    "epoch = 0\n",
    "\n",
    "target_model.load_state_dict(model.state_dict())\n",
    "\n",
    "while True:\n",
    "    epoch += 1\n",
    "    if epoch % 10 == 9:\n",
    "        new_epsilon = epsilon * epsilon_decay\n",
    "        print(new_epsilon)\n",
    "        if math.floor(new_epsilon) != math.floor(epsilon):\n",
    "            print(new_epsilon)\n",
    "            \n",
    "        epsilon = new_epsilon\n",
    "        \n",
    "    if epoch % 1000 == 999:\n",
    "        target_model.load_state_dict(model.state_dict())\n",
    "    #print('---------------------------------------------------------------------------')\n",
    "    node_state, link_state = env.get_state()  # 환경으로부터 실제 state를 관측해 옴(OMNeT++에서 얻어온 statistic로 대체되어야 함).\n",
    "    node_state = torch.tensor(node_state, dtype=torch.float)\n",
    "    link_state = torch.tensor(link_state, dtype=torch.float)\n",
    "    job_waiting_state = env.get_job_waiting_vector()\n",
    "    \n",
    "    env.init_job_progress()\n",
    "    temp_replay_memory = []\n",
    "    \n",
    "    # actions through multiple inferences\n",
    "    scheduled_job_num = 0\n",
    "    # 시스템에 job이 꽉찼거나, 스케줄할 job이 없거나, 현재 timestep에서 스케줄링한 job 개수가 10개 이상이면 그만한다.\n",
    "    while env.activated_job_num < 10 and len(env.job_waiting_queue) > 0 and scheduled_job_num < 10:\n",
    "        # job waiting 제일 앞에 있는 job 가져와서 스케줄링 해야 함.\n",
    "        job_idx = env.assign_index()\n",
    "        job = env.job_waiting_queue[0]\n",
    "        subtasks = job.subtasks\n",
    "        offloading_vector = []\n",
    "        \n",
    "        # 이 job의 모든 subtasks(layers)를 스케줄링해야 함.\n",
    "        for order in range(len(subtasks)):\n",
    "            network_state = Data(x=node_state, edge_attr=link_state, edge_index=adjacency)\n",
    "            prob = model.pi([network_state, torch.tensor(np.array([job_waiting_state]), dtype=torch.float)])\n",
    "            #print(prob)\n",
    "            #print(f'prob : {prob}')\n",
    "            \n",
    "            if random.random() > epsilon:\n",
    "                node = torch.argmax(prob[0])\n",
    "                \n",
    "            else:\n",
    "                node = random.randint(0, 4)\n",
    "            \n",
    "            #print(f'node : {node}')\n",
    "            offloading_vector.append(node)\n",
    "            \n",
    "            # state transition(환경으로부터 매번 state를 업데이트하는게 아닌, 현재 state를 기반으로 action에 해당하는 waiting만 더해줌).\n",
    "            # 아래의 구체적인 코드는 이해하시기 보단 OMNeT++에서 받아온 데이터로 새로 짜시는게 빠를 것 같습니다.\n",
    "            next_node_state = node_state.clone().detach()\n",
    "            next_job_waiting_state = job_waiting_state.copy()\n",
    "            \n",
    "            next_node_state[node][5*job.index+order] += (subtasks[order].comp_demand/100) # 100으로 나누는 이유 : 이렇게 해야 액션이 한쪽 노드로 쏠리게끔 학습이 되는 것을 어느정도 방지할 수 있는 것을 확인.\n",
    "            # 100으로 안나눠주면 너무 큰 값이 state에 추가되어서 inference시 가중치랑 곱해지면서 액션이 한쪽으로 확 쏠리는 걸로 예상됨.\n",
    "            next_network_state = Data(x=next_node_state, edge_attr=link_state, edge_index=adjacency)\n",
    "            next_job_waiting_state[5+order] = 0\n",
    "            \n",
    "            #model.put_data([network_state, job_waiting_state, node, 0, next_network_state, next_job_waiting_state, prob[0][node].item(), entropy])\n",
    "            temp_replay_memory.append([network_state, job_waiting_state, node, 0, next_network_state, next_job_waiting_state])\n",
    "            \n",
    "            node_state = next_node_state\n",
    "            job_waiting_state = next_job_waiting_state\n",
    "        \n",
    "        scheduled_job_num += 1\n",
    "        \n",
    "        # job을 시스템에 스케줄링.\n",
    "        env.schedule(job_idx, offloading_vector)\n",
    "        env.move_job() # backlog에 있는 job을 job waiting vector로 옮기는 것.\n",
    "        job_waiting_state = env.get_job_waiting_vector()\n",
    "    \n",
    "    \n",
    "    # 이 timestep이 끝날때까지 시간을 흘려줌(제가 만든 환경이므로 이런 코드가 필요합니다. OMNeT++은 그냥 시뮬레이션 하면서 시간이 자동으로 흐르므로 필요없습니다).\n",
    "    for ms in range(100):\n",
    "        env.step(time)\n",
    "        time += 1\n",
    "        \n",
    "        if time%20 == 0:\n",
    "            env.create_job(time)\n",
    "            env.move_job()\n",
    "    \n",
    "    # 이 timestep이 끝나면, 했던 행동에 대한 reward 계산(이 tiemstep 동안 시스템에 있는 job의 진행정도)\n",
    "    # 지금 보상 계산하는 과정에서 살짝 오류가 있습니다. 보상을 계산하기 전에 이번 timestep에서 어떤 job이 끝나서 없어져 버리면 그 job에 대한 진행정도를 추적할 수 없어서 0이 됩니다.\n",
    "    # 여기서 보상을 계산하는 부분(즉, job의 진행정도 tracking)이 환경과 좀 관련 있기 때문에, 보상을 어떻게 계산할지를 새로 짜시는게 빠를 것 같습니다.\n",
    "    per_timestep_reward = env.get_reward()\n",
    "    #if per_timestep_reward == 0:\n",
    "    #    per_timestep_reward = 5\n",
    "    \n",
    "    #print(f'reward : {per_timestep_reward}')\n",
    "    if time > 100:\n",
    "        reward_history.append(per_timestep_reward)\n",
    "        \n",
    "    if time % 5000 == 0:\n",
    "        figure(figsize=(30, 10))\n",
    "        plt.plot(reward_history)\n",
    "        plt.show()\n",
    "    \n",
    "    for i in range(len(temp_replay_memory)):\n",
    "        temp_replay_memory[i][3] = per_timestep_reward\n",
    "        \n",
    "    model.replay_memory += temp_replay_memory\n",
    "    model.replay_memory = model.replay_memory[-50000:]\n",
    "    if time % 1000 == 0:\n",
    "        model.train_net(target_model)\n",
    "    \n",
    "    # 새로운 job을 발생시키는 코드.\n",
    "    if time%10 == 0:\n",
    "        env.create_job(time)\n",
    "        env.move_job()\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4131cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.replay_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed1899d",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_batch, job_waiting, a, r, next_network_batch, next_job_waiting = model.make_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1369c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omnetTest",
   "language": "python",
   "name": "omnettest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
